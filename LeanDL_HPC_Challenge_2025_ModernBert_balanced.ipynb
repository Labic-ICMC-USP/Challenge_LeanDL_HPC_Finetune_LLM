{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30960be7",
      "metadata": {},
      "source": [
        "# Importando dados processados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e38c066d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kenzosaki/miniconda3/envs/bonito/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pickle \n",
        "import datasets\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba7a4efe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4.0.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "10c3f8f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"/exp/kenzosaki/data/LeanDL-HPC/my_data.pickle\", \"rb\") as file:\n",
        "    split_train = pickle.load(file)\n",
        "    split_eval = pickle.load(file)\n",
        "    split_test = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "975a8189",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = split_train.to_pandas()\n",
        "eval_df = split_eval.to_pandas()\n",
        "test_df = split_test.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iVZ8OoyLAvRj",
      "metadata": {
        "id": "iVZ8OoyLAvRj"
      },
      "source": [
        "# Exploração dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "RCE780PNAwg2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCE780PNAwg2",
        "outputId": "0151cf27-b209-4af9-e78f-4c5778131efb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['hash_id', 'tema_id', 'tema', 'palavras_chave', 'uf_tema_info',\n",
              "       'uf_pesquisador', 'nome_programa', 'sigla_entidade_ensino',\n",
              "       'nome_producao', 'nome_subtipo_producao', 'nome_area_concentracao',\n",
              "       'nome_linha_pesquisa', 'nome_projeto', 'descricao_palavra_chave',\n",
              "       'descricao_resumo', 'descricao_abstract', 'descricao_keyword',\n",
              "       'data_titulacao', 'nome_grau_academico',\n",
              "       'nome_grande_area_conhecimento', 'nome_area_conhecimento',\n",
              "       'nome_subarea_conhecimento', 'nome_especialidade', 'modelo_nivel',\n",
              "       'modelo_explicacao', '__index_level_0__'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dz2g9LYUA3Kh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz2g9LYUA3Kh",
        "outputId": "789d2409-99c7-4bdf-8feb-9d753e034911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(437, 33620)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['tema'].nunique(), len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e0SNbR3RCKII",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "e0SNbR3RCKII",
        "outputId": "f662d490-b79a-4a5d-8f3d-ca79f9dfed45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "modelo_nivel\n",
              "BAIXA    0.478614\n",
              "MEDIA    0.308418\n",
              "ALTA     0.212968\n",
              "Name: count, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['modelo_nivel'].value_counts()/len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f282679c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: remover\n",
        "# train_df = train_df.sample(1000, random_state=2025) # para teste"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1iqF6Tf6CS_N",
      "metadata": {
        "id": "1iqF6Tf6CS_N"
      },
      "source": [
        "# Preparando inputs para o ModernBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "EykZ11UjKKc0",
      "metadata": {
        "id": "EykZ11UjKKc0"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lvNLUWKzH0Rn",
      "metadata": {
        "id": "lvNLUWKzH0Rn"
      },
      "outputs": [],
      "source": [
        "train_df['palavras_chave'] = train_df['palavras_chave'].apply(lambda array: \", \".join(array.tolist()))\n",
        "eval_df['palavras_chave'] = eval_df['palavras_chave'].apply(lambda array: \", \".join(array.tolist()))\n",
        "test_df['palavras_chave'] = test_df['palavras_chave'].apply(lambda array: \", \".join(array.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ySG8oDqqBgeC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySG8oDqqBgeC",
        "outputId": "fabb8c6c-98b9-4818-f3fb-5502560158ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'nome_projeto': 'Nome projeto',\n",
              " 'descricao_palavra_chave': 'Descricao palavra chave',\n",
              " 'descricao_resumo': 'Descricao resumo',\n",
              " 'tema': 'Tema',\n",
              " 'palavras_chave': 'Palavras chave'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input_cols = ['nome_producao', 'nome_area_concentracao', 'nome_linha_pesquisa', 'nome_projeto', 'descricao_palavra_chave', 'descricao_resumo', 'tema', 'palavras_chave']\n",
        "input_cols = ['nome_projeto', 'descricao_palavra_chave', 'descricao_resumo', 'tema', 'palavras_chave']\n",
        "input_cols_mapping = {col: col.replace('_', ' ').capitalize() for col in input_cols}\n",
        "input_cols_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "uWu9mlyaD1Ti",
      "metadata": {
        "id": "uWu9mlyaD1Ti"
      },
      "outputs": [],
      "source": [
        "def create_input_from_row(row: pd.Series) -> str:\n",
        "  input_str = \"\"\n",
        "\n",
        "  for col, formated_name in input_cols_mapping.items():\n",
        "    input_str += f\"{formated_name}: {row[col].capitalize()}\\n\"\n",
        "\n",
        "  return input_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "FtB4igZvIiVt",
      "metadata": {
        "id": "FtB4igZvIiVt"
      },
      "outputs": [],
      "source": [
        "train_df[\"bert_raw_inputs\"] = [create_input_from_row(row) for _, row in train_df.iterrows()]\n",
        "eval_df[\"bert_raw_inputs\"] = [create_input_from_row(row) for _, row in eval_df.iterrows()]\n",
        "test_df[\"bert_raw_inputs\"] = [create_input_from_row(row) for _, row in test_df.iterrows()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27P5Ts_8IuSZ",
      "metadata": {
        "id": "27P5Ts_8IuSZ"
      },
      "source": [
        "# Splits de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "RIlrJ-EOIuII",
      "metadata": {
        "id": "RIlrJ-EOIuII"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sPwWkjroM-bh",
      "metadata": {
        "id": "sPwWkjroM-bh"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_df['modelo_nivel'] = label_encoder.fit_transform(train_df['modelo_nivel'])\n",
        "eval_df['modelo_nivel'] = label_encoder.transform(eval_df['modelo_nivel'])\n",
        "test_df['modelo_nivel'] = label_encoder.transform(test_df['modelo_nivel'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SleSlvubK278",
      "metadata": {
        "id": "SleSlvubK278"
      },
      "source": [
        "# Hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "zdEMKZeCK43k",
      "metadata": {
        "id": "zdEMKZeCK43k"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, ModernBertForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "uwZWOI4jK7uR",
      "metadata": {
        "id": "uwZWOI4jK7uR"
      },
      "outputs": [],
      "source": [
        "train_ds = Dataset.from_dict(\n",
        "    {\n",
        "        \"text\": train_df['bert_raw_inputs'].tolist(),\n",
        "        \"label\": train_df['modelo_nivel'].tolist()\n",
        "    }\n",
        ")\n",
        "\n",
        "eval_ds = Dataset.from_dict(\n",
        "    {\n",
        "        \"text\": eval_df['bert_raw_inputs'].tolist(),\n",
        "        \"label\": eval_df['modelo_nivel'].tolist()\n",
        "    }\n",
        ")\n",
        "\n",
        "test_ds = Dataset.from_dict(\n",
        "    {\n",
        "        \"text\": test_df['bert_raw_inputs'].tolist(),\n",
        "        \"label\": test_df['modelo_nivel'].tolist()\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ZjGf0N6lLIB6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjGf0N6lLIB6",
        "outputId": "63e40c0d-4fc5-47a5-9a14-b146cd34db56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN = 1024\n",
        "model = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, truncation_side='left')\n",
        "model = ModernBertForSequenceClassification.from_pretrained(model, num_labels=3, max_position_embeddings=MAX_LEN) # talvez 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8cHVGjvzLodI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cHVGjvzLodI",
        "outputId": "05bc94d7-94a5-4c84-83cf-d4d626103841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModernBertForSequenceClassification(\n",
              "  (model): ModernBertModel(\n",
              "    (embeddings): ModernBertEmbeddings(\n",
              "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
              "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ModernBertEncoderLayer(\n",
              "        (attn_norm): Identity()\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1-21): 21 x ModernBertEncoderLayer(\n",
              "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (head): ModernBertPredictionHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (act): GELUActivation()\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7UkE1TRPQkYY",
      "metadata": {
        "id": "7UkE1TRPQkYY"
      },
      "source": [
        "# Analisando número de tokens no conjunto de dados inteiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aNIqaU3RRa5S",
      "metadata": {
        "id": "aNIqaU3RRa5S"
      },
      "outputs": [],
      "source": [
        "def get_num_tokens(text: str) -> int:\n",
        "  return len(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "Vvewyel9SUvD",
      "metadata": {
        "id": "Vvewyel9SUvD"
      },
      "outputs": [],
      "source": [
        "sampled_df = train_df.sample(1000, random_state=2025)\n",
        "# sampled_df = dados_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "thtE_d8qMA4-",
      "metadata": {
        "id": "thtE_d8qMA4-"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"num_tokens\"] = sampled_df[\"bert_raw_inputs\"].apply(get_num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "mHM-OrCwSdwJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "mHM-OrCwSdwJ",
        "outputId": "731e708c-0c5b-4d0b-8094-4aab2697ad62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean      910.950000\n",
              "std       270.799701\n",
              "min       241.000000\n",
              "25%       731.750000\n",
              "50%       876.000000\n",
              "75%      1068.500000\n",
              "max      4304.000000\n",
              "Name: num_tokens, dtype: float64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df[\"num_tokens\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6Xop_33gShAS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "6Xop_33gShAS",
        "outputId": "66e4c735-0976-4c25-ce9c-1bf8ea8889e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean      910.950000\n",
              "std       270.799701\n",
              "min       241.000000\n",
              "25%       731.750000\n",
              "50%       876.000000\n",
              "75%      1068.500000\n",
              "max      4304.000000\n",
              "Name: num_tokens, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df[\"num_tokens\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4d5841b2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1021.3)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df[\"num_tokens\"].quantile(0.70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cu0JZOAhd7_w",
      "metadata": {
        "id": "Cu0JZOAhd7_w"
      },
      "source": [
        "# Pre-processamento de Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "xPWjDUH2deuR",
      "metadata": {
        "id": "xPWjDUH2deuR"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5ir27h0xcmUI",
      "metadata": {
        "id": "5ir27h0xcmUI"
      },
      "outputs": [],
      "source": [
        "def tokenizer_function(example):\n",
        "  # Em caso de uma tarefa de classificação de pares de texto, modificar este valor de retorno\n",
        "  # truncation=True, padding=\"max_length\", max_length=123 para truncar e padronizar os tamanhos de tokens!!!\n",
        "  return tokenizer(\n",
        "      example[\"text\"], truncation=True, max_length=MAX_LEN\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "stgVH7H2c-mA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fd2a334c9801427bad43d0f577909373",
            "43a1c8c0ea054197a64423b736920a8e",
            "2d770cacc1c643048bd07fb695d9feab",
            "dbc66082c62b490fa51a30de84fb93ed",
            "1337d826cc1c432fb0ff426f76aa0a40",
            "c222a8bf0073423a8065d3ec330d5ee8",
            "833a450e4a914a68beeeac27ef2e808c",
            "7c901c88a5184cf49be3c99d2130d994",
            "7f2c3f88e34f421ca1c2f724b9a145f9",
            "eea7e6f921aa4311bdfd10973dc80211",
            "a6da50e3375f4dcdb9e7975e8fe07813",
            "6f3e73f3dad441cdbd7e94497543fa66",
            "6eb3b183113d42f5a44dcec5cbd1bb89",
            "3128da4fe1c64711ae51801d3ff854e2",
            "73a96be96b074ad5997ae68b5e2eb8ce",
            "0e046339f93946889a718fc04abb72d2",
            "e4626f82fbe5407ba5fba7f63cfec5f6",
            "2eee9befa38348a58e9e1f89be177aa5",
            "e6b279d9b02f4a0e9f670243083008b0",
            "e668ba65bdc342b5b749c7146f6f9f27",
            "18e619f6680440adb94aa980b1e89f97",
            "aeec25216770453cbf08207b00eb58a1"
          ]
        },
        "id": "stgVH7H2c-mA",
        "outputId": "c34c50ac-8390-4b36-e7d0-4bc35a730107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 33620/33620 [00:06<00:00, 5174.78 examples/s]\n",
            "Map: 100%|██████████| 4203/4203 [00:00<00:00, 5172.55 examples/s]\n",
            "Map: 100%|██████████| 4203/4203 [00:00<00:00, 5345.13 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenizando todos os elementos do conjunto de dados em batches\n",
        "train_ds = train_ds.map(tokenizer_function, batched=True)\n",
        "eval_ds = eval_ds.map(tokenizer_function, batched=True)\n",
        "test_ds = test_ds.map(tokenizer_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "qyrSEjnbdEVc",
      "metadata": {
        "id": "qyrSEjnbdEVc"
      },
      "outputs": [],
      "source": [
        "# Mantendo apenas colunas cujos nomes coincidam com os métodos forward dos transformers\n",
        "train_ds = train_ds.remove_columns(column_names=[\"text\"])\n",
        "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
        "train_ds = train_ds.with_format(\"torch\")\n",
        "\n",
        "eval_ds = eval_ds.remove_columns(column_names=[\"text\"])\n",
        "eval_ds = eval_ds.rename_column(\"label\", \"labels\")\n",
        "eval_ds = eval_ds.with_format(\"torch\")\n",
        "\n",
        "test_ds = test_ds.remove_columns(column_names=[\"text\"])\n",
        "test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
        "test_ds = test_ds.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "92nWl1g0dZZ_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92nWl1g0dZZ_",
        "outputId": "d45d2164-eccf-4060-b2ae-e4eac083c5d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 33620\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "GlZXmH-xddW6",
      "metadata": {
        "id": "GlZXmH-xddW6"
      },
      "outputs": [],
      "source": [
        "# Collator para Dynamic Padding\n",
        "collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "29aa9995",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS]idismo cultural. o objetivo da pesquisa foi elaborar heurísticas para a criação do produto de moda para sustentabilidade com preceitos do hibridismo cultural. para isso, pesquisou-se por meio de revisões narrativas e sistemáticas a fim de encontrar os principais conceitos, ênfases e lacunas das temáticas que permeiam o estudo. os resultados obtidos das revisões foram trabalhos que predominantemente discutem as temáticas de maneira separada, ou enfocam apenas em análises de construção do vestuário, ou discorrem sobre a percepção do hibridismo na moda, ou, ainda, expõem cenários para aplicação de projetos sustentáveis pela moda. portanto, encontrou-se uma interseção entre as temáticas a serem exploradas. para tanto, utilizou-se o método heurístico, que busca no conhecimento tácito os indícios para a emersão do conhecimento sistematizado. as etapas do método constituem os ciclos heurísticos, onde na observação de processos criativos da moda autoral buscou-se os dados a serem analisados, junto com a realização de entrevistas contextuais remotas, que foram transformadas em dados e posteriormente confrontadas com a teoria existente a fim de elicitar as novas heurísticas. houve nove ciclos heurísticos nas regiões norte, nordeste, sudeste e sul do brasil. após os ciclos heurísticos, através da transcrição e dos destaques das entrevistas, verificou-se os indícios de hibridização e as principais práticas sustentáveis. notou-se, sobretudo, o fenômeno da adaptação, que esteve presente em todas as regiões pesquisadas, assim, encontrou-se consonância ao design vernacular. entendeu-se que os demais indícios contribuem para a originalidade do criador de moda autoral e para reflexão sobre as particularidades culturais no país. dessas práticas também estruturou-se um quadro com as heurísticas mais identificadas, assim como a compreensão de que nem todas as teorias pré-existentes alcançam as atividades da moda autoral. como última etapa, a partir dos quadros dos ciclos heurísticos, sintetizou-se novas heurísticas a partir da repetição dos fenômenos. as heurísticas sintetizadas foram transformadas em cartões visuais para aplicação de workshop remoto com os entrevistados dos nove ciclos anteriores. após as discussões sobre moda autoral e sustentabilidade, houve uma recepção positiva apoiada pelos cartões com heurísticas, no momento cocriou-se mais uma heurística durante o workshop. concluiu-se, portanto, que os objetivos foram alcançados, pois, a rica fonte de dados contribuiu para sistematizar o conhecimento tácito, percebeu-se através do método que a moda autoral é intrínseca à sustentabilidade uma vez que, suas práticas corresponderam às diretrizes existentes e pôde-se estabelecer novas teorias. por fim, de modo a ampliar a usabilidade das heurísticas levantadas e auxiliar a autoria de moda sustentável, criou-se cartões visuais com todas as heurísticas sintetizadas e as mais identificadas.\\nTema: Economia criativa: cultura, arte e lazer\\nPalavras chave: Artesanato, audiovisual, cultura de inovação, cultura e arte, desenvolvimento sustentável, economia criativa, educação e capacitação, empreendedorismo criativo, fomento cultural, inclusão social, indústria criativa, música, patrimônio cultural, tecnologia e inovação, turismo cultural\\n[SEP]'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(train_ds[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75e3cae",
      "metadata": {},
      "source": [
        "# Implementando trainer com loss customizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ecb9858f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Any, Optional, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cb135abc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "modelo_nivel\n",
              "0     7160\n",
              "1    16091\n",
              "2    10369\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['modelo_nivel'].value_counts().loc[[0,1,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6e27888f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# inverso da frequência (ajustado para só 3 classes)\n",
        "class_counts = torch.tensor(train_df['modelo_nivel'].value_counts().loc[[0,1,2]].values, dtype=torch.float, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights = class_weights / class_weights.sum() * len(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5a64047c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supondo que você já tenha contado os exemplos por classe\n",
        "# class_counts = [100, 300, 50]  # Exemplo para 3 classes\n",
        "# class_weights = [1.0 / c for c in class_counts]  # inversamente proporcional à frequência\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch: Optional[torch.Tensor] = None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, labels) # usa a loss_fn definida acima\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GvEYK0M5ecfH",
      "metadata": {
        "id": "GvEYK0M5ecfH"
      },
      "source": [
        "# Configurando Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "w8Exwa5TebcJ",
      "metadata": {
        "id": "w8Exwa5TebcJ"
      },
      "outputs": [],
      "source": [
        "from transformers import get_cosine_schedule_with_warmup, get_constant_schedule\n",
        "from torch.optim import AdamW\n",
        "from transformers import EarlyStoppingCallback\n",
        "from codecarbon import EmissionsTracker # para calcular emissões de CO2\n",
        "from transformers import set_seed\n",
        "import math\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qf6N__Geh-l",
      "metadata": {
        "id": "7qf6N__Geh-l"
      },
      "outputs": [],
      "source": [
        "balanced_loss = True\n",
        "bs = 32\n",
        "acc_steps = 1\n",
        "epochs = 10\n",
        "lr = 2e-4\n",
        "wd = 1e-3\n",
        "patience = 2\n",
        "seed = 42\n",
        "crit = 'f1'\n",
        "evals_per_epoch = 1\n",
        "output_dir = f\"/exp/kenzosaki/data/LeanDL-HPC/models/balanced_modern_bert_{epochs}epochs_{bs}bs_{lr}lr_{wd}wd_{acc_steps}accsteps/\"\n",
        "train_code_carbon_out = 'balanced_modernbert_fine_tuning_emissions_training.csv'\n",
        "inference_code_carbon_out = 'balanced_modernbert_fine_tuning_emissions_inference.csv'\n",
        "\n",
        "warmup_steps = math.ceil((len(train_ds)/bs) * epochs * 0.1) #10% of train data for warm-up\n",
        "train_steps = int(epochs * len(train_ds)/bs)\n",
        "es_callback = EarlyStoppingCallback(early_stopping_patience=patience)\n",
        "\n",
        "final_bs = bs * acc_steps\n",
        "epoch_steps = math.ceil(len(train_ds)/final_bs)\n",
        "train_steps = int(epochs * epoch_steps)\n",
        "eval_steps = int(epoch_steps / evals_per_epoch) # evaluating every 10% of the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "cDj3aYrdevuM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDj3aYrdevuM",
        "outputId": "828a4880-b5e7-4d98-d06b-fc6a44c985a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1051, 10510)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "warmup_steps, train_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "FusH0lqPeydx",
      "metadata": {
        "id": "FusH0lqPeydx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "# from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metrics_dict = classification_report(labels, predictions, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": metrics_dict[\"accuracy\"],\n",
        "        \"precision\": metrics_dict[\"macro avg\"][\"precision\"],\n",
        "        \"recall\": metrics_dict[\"macro avg\"][\"recall\"],\n",
        "        \"f1\": metrics_dict[\"macro avg\"][\"f1-score\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "SOBpncfLbAQN",
      "metadata": {
        "id": "SOBpncfLbAQN"
      },
      "outputs": [],
      "source": [
        "# optimizer = AdamW(model.parameters(), lr=lr)\n",
        "# # scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=train_steps, num_cycles=0.5)\n",
        "# scheduler = get_constant_schedule(optimizer) # testar este tbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5f713b",
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WUwiB8mKfgqd",
      "metadata": {
        "id": "WUwiB8mKfgqd"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=epochs,                                                                              # total number of training epochs\n",
        "    per_device_train_batch_size=bs,                                                                       # batch size per device during training\n",
        "    per_device_eval_batch_size=bs,                                                                        # batch size for evaluation\n",
        "    # warmup_steps=warmup_steps,                                                                            # number of warmup steps for learning rate scheduler\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",                                                                                # evaluation interval\n",
        "    logging_dir=f\"{output_dir}/logs\",                                                                                 # directory for storing logs\n",
        "    save_total_limit=patience+1,                                                                              # checkpoint save interval\n",
        "    report_to='none',\n",
        "    gradient_accumulation_steps=acc_steps,\n",
        "    metric_for_best_model=crit,\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=True,\n",
        "    learning_rate=lr,\n",
        "    weight_decay=wd,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=100,\n",
        "    logging_strategy='steps',\n",
        "    log_level=\"info\",\n",
        "    eval_steps=eval_steps,\n",
        "    save_steps=eval_steps,\n",
        "    seed=seed\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "QFdvyS8af-L5",
      "metadata": {
        "id": "QFdvyS8af-L5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using auto half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using balanced loss with class weights: tensor([1.4048, 0.6251, 0.9701], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "if balanced_loss:\n",
        "    print(f\"Using balanced loss with class weights: {class_weights}\")\n",
        "    trainer = WeightedTrainer(\n",
        "        model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        # optimizers=(optimizer, scheduler),\n",
        "        data_collator=collator\n",
        "    )\n",
        "else:\n",
        "    print(f\"Using standard loss\")\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        # optimizers=(optimizer, scheduler),\n",
        "        data_collator=collator\n",
        "    )\n",
        "\n",
        "trainer.add_callback(es_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "76814508",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "modelo_nivel\n",
              "0     7160\n",
              "1    16091\n",
              "2    10369\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['modelo_nivel'].value_counts().loc[[0,1,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jsWGY8XvgC1Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "jsWGY8XvgC1Y",
        "outputId": "9f4cb8a0-5755-40d4-e7cd-ee9081a940da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 13:19:30] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 13:19:30] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 13:19:30] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 13:19:31] We saw that you have a Intel(R) Core(TM) i9-14900KF but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:19:31] We will use the default power consumption of 4 W per thread for your 32 CPU, so 128W.\n",
            "[codecarbon WARNING @ 13:19:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 13:19:31] CPU Model on constant consumption mode: Intel(R) Core(TM) i9-14900KF\n",
            "[codecarbon WARNING @ 13:19:31] No CPU tracking mode found. Falling back on CPU load mode.\n",
            "[codecarbon INFO @ 13:19:31] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 13:19:31] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 13:19:31] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: cpu_load\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 13:19:31] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 13:19:31]   Platform system: Linux-6.14.0-28-generic-x86_64-with-glibc2.39\n",
            "[codecarbon INFO @ 13:19:31]   Python version: 3.13.5\n",
            "[codecarbon INFO @ 13:19:31]   CodeCarbon version: 3.0.4\n",
            "[codecarbon INFO @ 13:19:31]   Available RAM : 125.634 GB\n",
            "[codecarbon INFO @ 13:19:31]   CPU count: 32 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 13:19:31]   CPU model: Intel(R) Core(TM) i9-14900KF\n",
            "[codecarbon INFO @ 13:19:31]   GPU count: 1\n",
            "[codecarbon INFO @ 13:19:31]   GPU model: 1 x NVIDIA RTX A5000\n",
            "[codecarbon INFO @ 13:19:34] Emissions data (if any) will be saved to file /home/kenzosaki/repos/challenge_bonito/balanced_modernbert_fine_tuning_emissions_trainer.csv\n",
            "***** Running training *****\n",
            "  Num examples = 33,620\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10,510\n",
            "  Number of trainable parameters = 149,607,171\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='332' max='10510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  332/10510 05:27 < 2:48:19, 1.01 it/s, Epoch 0.31/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 13:19:50] Energy consumed for RAM : 0.000164 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:19:50] Delta energy consumed for CPU with cpu_load : 0.000055 kWh, power : 12.805211970560002 W\n",
            "[codecarbon INFO @ 13:19:50] Energy consumed for All CPU : 0.000055 kWh\n",
            "[codecarbon INFO @ 13:19:50] Energy consumed for all GPUs : 0.000909 kWh. Total GPU Power : 204.37357853564714 W\n",
            "[codecarbon INFO @ 13:19:50] 0.001127 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:20:05] Energy consumed for RAM : 0.000317 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:20:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8053388792 W\n",
            "[codecarbon INFO @ 13:20:05] Energy consumed for All CPU : 0.000107 kWh\n",
            "[codecarbon INFO @ 13:20:05] Energy consumed for all GPUs : 0.001842 kWh. Total GPU Power : 224.02428039356613 W\n",
            "[codecarbon INFO @ 13:20:05] 0.002265 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:20:20] Energy consumed for RAM : 0.000470 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:20:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8051483816 W\n",
            "[codecarbon INFO @ 13:20:20] Energy consumed for All CPU : 0.000158 kWh\n",
            "[codecarbon INFO @ 13:20:20] Energy consumed for all GPUs : 0.002780 kWh. Total GPU Power : 225.15907443043352 W\n",
            "[codecarbon INFO @ 13:20:20] 0.003408 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:20:35] Energy consumed for RAM : 0.000623 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:20:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.805146869600001 W\n",
            "[codecarbon INFO @ 13:20:35] Energy consumed for All CPU : 0.000210 kWh\n",
            "[codecarbon INFO @ 13:20:35] Energy consumed for all GPUs : 0.003715 kWh. Total GPU Power : 224.4531620830023 W\n",
            "[codecarbon INFO @ 13:20:35] 0.004548 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:20:50] Energy consumed for RAM : 0.000776 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:20:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8048181608 W\n",
            "[codecarbon INFO @ 13:20:50] Energy consumed for All CPU : 0.000261 kWh\n",
            "[codecarbon INFO @ 13:20:50] Energy consumed for all GPUs : 0.004652 kWh. Total GPU Power : 224.87239631708178 W\n",
            "[codecarbon INFO @ 13:20:50] 0.005689 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:21:05] Energy consumed for RAM : 0.000929 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:21:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.805380581600003 W\n",
            "[codecarbon INFO @ 13:21:05] Energy consumed for All CPU : 0.000313 kWh\n",
            "[codecarbon INFO @ 13:21:05] Energy consumed for all GPUs : 0.005591 kWh. Total GPU Power : 225.2505344203759 W\n",
            "[codecarbon INFO @ 13:21:05] 0.006832 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:21:20] Energy consumed for RAM : 0.001082 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:21:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8052160544 W\n",
            "[codecarbon INFO @ 13:21:20] Energy consumed for All CPU : 0.000365 kWh\n",
            "[codecarbon INFO @ 13:21:20] Energy consumed for all GPUs : 0.006535 kWh. Total GPU Power : 226.77272266095596 W\n",
            "[codecarbon INFO @ 13:21:20] 0.007982 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:21:35] Energy consumed for RAM : 0.001235 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:21:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8051453144 W\n",
            "[codecarbon INFO @ 13:21:35] Energy consumed for All CPU : 0.000416 kWh\n",
            "[codecarbon INFO @ 13:21:35] Energy consumed for all GPUs : 0.007481 kWh. Total GPU Power : 227.0365091775087 W\n",
            "[codecarbon INFO @ 13:21:35] 0.009132 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:21:35] 0.007422 g.CO2eq/s mean an estimation of 234.06453713497203 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:21:50] Energy consumed for RAM : 0.001388 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:21:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.805971449600001 W\n",
            "[codecarbon INFO @ 13:21:50] Energy consumed for All CPU : 0.000468 kWh\n",
            "[codecarbon INFO @ 13:21:50] Energy consumed for all GPUs : 0.008425 kWh. Total GPU Power : 226.3701118346941 W\n",
            "[codecarbon INFO @ 13:21:50] 0.010280 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:22:05] Energy consumed for RAM : 0.001541 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:22:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8061328232 W\n",
            "[codecarbon INFO @ 13:22:05] Energy consumed for All CPU : 0.000519 kWh\n",
            "[codecarbon INFO @ 13:22:05] Energy consumed for all GPUs : 0.009370 kWh. Total GPU Power : 227.03240761174274 W\n",
            "[codecarbon INFO @ 13:22:05] 0.011431 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:22:20] Energy consumed for RAM : 0.001694 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:22:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8049240728 W\n",
            "[codecarbon INFO @ 13:22:20] Energy consumed for All CPU : 0.000571 kWh\n",
            "[codecarbon INFO @ 13:22:20] Energy consumed for all GPUs : 0.010317 kWh. Total GPU Power : 227.1115846094956 W\n",
            "[codecarbon INFO @ 13:22:20] 0.012581 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:22:35] Energy consumed for RAM : 0.001847 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:22:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.804762245600001 W\n",
            "[codecarbon INFO @ 13:22:35] Energy consumed for All CPU : 0.000622 kWh\n",
            "[codecarbon INFO @ 13:22:35] Energy consumed for all GPUs : 0.011262 kWh. Total GPU Power : 226.83545954127038 W\n",
            "[codecarbon INFO @ 13:22:35] 0.013731 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:22:50] Energy consumed for RAM : 0.002000 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:22:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.804921092000003 W\n",
            "[codecarbon INFO @ 13:22:50] Energy consumed for All CPU : 0.000674 kWh\n",
            "[codecarbon INFO @ 13:22:50] Energy consumed for all GPUs : 0.012206 kWh. Total GPU Power : 226.65207978795723 W\n",
            "[codecarbon INFO @ 13:22:50] 0.014880 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:23:05] Energy consumed for RAM : 0.002153 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:23:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8055987776 W\n",
            "[codecarbon INFO @ 13:23:05] Energy consumed for All CPU : 0.000726 kWh\n",
            "[codecarbon INFO @ 13:23:05] Energy consumed for all GPUs : 0.013158 kWh. Total GPU Power : 228.42043491462283 W\n",
            "[codecarbon INFO @ 13:23:05] 0.016036 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:23:20] Energy consumed for RAM : 0.002306 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:23:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8051483816 W\n",
            "[codecarbon INFO @ 13:23:20] Energy consumed for All CPU : 0.000777 kWh\n",
            "[codecarbon INFO @ 13:23:20] Energy consumed for all GPUs : 0.014103 kWh. Total GPU Power : 226.90552765625205 W\n",
            "[codecarbon INFO @ 13:23:20] 0.017186 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:23:35] Energy consumed for RAM : 0.002459 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:23:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8057946824 W\n",
            "[codecarbon INFO @ 13:23:35] Energy consumed for All CPU : 0.000829 kWh\n",
            "[codecarbon INFO @ 13:23:35] Energy consumed for all GPUs : 0.015045 kWh. Total GPU Power : 226.03553586495065 W\n",
            "[codecarbon INFO @ 13:23:35] 0.018332 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:23:35] 0.007540 g.CO2eq/s mean an estimation of 237.77269012366762 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:23:50] Energy consumed for RAM : 0.002612 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:23:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.805001249600002 W\n",
            "[codecarbon INFO @ 13:23:50] Energy consumed for All CPU : 0.000880 kWh\n",
            "[codecarbon INFO @ 13:23:50] Energy consumed for all GPUs : 0.015988 kWh. Total GPU Power : 226.4715783802187 W\n",
            "[codecarbon INFO @ 13:23:50] 0.019480 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:24:05] Energy consumed for RAM : 0.002765 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:24:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8065166408 W\n",
            "[codecarbon INFO @ 13:24:05] Energy consumed for All CPU : 0.000932 kWh\n",
            "[codecarbon INFO @ 13:24:05] Energy consumed for all GPUs : 0.016931 kWh. Total GPU Power : 226.1856772611517 W\n",
            "[codecarbon INFO @ 13:24:05] 0.020627 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:24:20] Energy consumed for RAM : 0.002918 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:24:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.806071644800003 W\n",
            "[codecarbon INFO @ 13:24:20] Energy consumed for All CPU : 0.000983 kWh\n",
            "[codecarbon INFO @ 13:24:20] Energy consumed for all GPUs : 0.017874 kWh. Total GPU Power : 226.44400530888845 W\n",
            "[codecarbon INFO @ 13:24:20] 0.021775 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:24:35] Energy consumed for RAM : 0.003071 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:24:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8049739832 W\n",
            "[codecarbon INFO @ 13:24:35] Energy consumed for All CPU : 0.001035 kWh\n",
            "[codecarbon INFO @ 13:24:35] Energy consumed for all GPUs : 0.018818 kWh. Total GPU Power : 226.577104143203 W\n",
            "[codecarbon INFO @ 13:24:35] 0.022924 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:24:50] Energy consumed for RAM : 0.003224 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:24:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8057851928 W\n",
            "[codecarbon INFO @ 13:24:50] Energy consumed for All CPU : 0.001087 kWh\n",
            "[codecarbon INFO @ 13:24:50] Energy consumed for all GPUs : 0.019763 kWh. Total GPU Power : 226.83563065102044 W\n",
            "[codecarbon INFO @ 13:24:50] 0.024074 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:25:05] Energy consumed for RAM : 0.003377 kWh. RAM Power : 38.0 W\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tracker = EmissionsTracker(output_file=code_carbon_out)\n\u001b[32m      3\u001b[39m tracker.start()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m emissions = tracker.stop()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal de emissões (detalhes em emissions.csv): \u001b[39m\u001b[33m\"\u001b[39m,emissions)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/transformers/trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bonito/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 13:25:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8119196864 W\n",
            "[codecarbon INFO @ 13:25:05] Energy consumed for All CPU : 0.001138 kWh\n",
            "[codecarbon INFO @ 13:25:05] Energy consumed for all GPUs : 0.020709 kWh. Total GPU Power : 226.8982889683421 W\n",
            "[codecarbon INFO @ 13:25:05] 0.025224 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:25:20] Energy consumed for RAM : 0.003530 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:25:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.801489032000001 W\n",
            "[codecarbon INFO @ 13:25:20] Energy consumed for All CPU : 0.001190 kWh\n",
            "[codecarbon INFO @ 13:25:20] Energy consumed for all GPUs : 0.021044 kWh. Total GPU Power : 80.40982664245486 W\n",
            "[codecarbon INFO @ 13:25:20] 0.025763 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:25:35] Energy consumed for RAM : 0.003683 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:25:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8023051088 W\n",
            "[codecarbon INFO @ 13:25:35] Energy consumed for All CPU : 0.001241 kWh\n",
            "[codecarbon INFO @ 13:25:35] Energy consumed for all GPUs : 0.021145 kWh. Total GPU Power : 24.411898526244777 W\n",
            "[codecarbon INFO @ 13:25:35] 0.026070 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:25:35] 0.006341 g.CO2eq/s mean an estimation of 199.96962563583457 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:25:50] Energy consumed for RAM : 0.003836 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:25:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800459014400003 W\n",
            "[codecarbon INFO @ 13:25:50] Energy consumed for All CPU : 0.001293 kWh\n",
            "[codecarbon INFO @ 13:25:50] Energy consumed for all GPUs : 0.021206 kWh. Total GPU Power : 14.517455104659382 W\n",
            "[codecarbon INFO @ 13:25:50] 0.026335 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:26:05] Energy consumed for RAM : 0.003989 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:26:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800017913600001 W\n",
            "[codecarbon INFO @ 13:26:05] Energy consumed for All CPU : 0.001344 kWh\n",
            "[codecarbon INFO @ 13:26:05] Energy consumed for all GPUs : 0.021244 kWh. Total GPU Power : 9.239683295143001 W\n",
            "[codecarbon INFO @ 13:26:05] 0.026578 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:26:20] Energy consumed for RAM : 0.004142 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:26:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000331200001 W\n",
            "[codecarbon INFO @ 13:26:20] Energy consumed for All CPU : 0.001396 kWh\n",
            "[codecarbon INFO @ 13:26:20] Energy consumed for all GPUs : 0.021271 kWh. Total GPU Power : 6.458769830326231 W\n",
            "[codecarbon INFO @ 13:26:20] 0.026809 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:26:35] Energy consumed for RAM : 0.004295 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:26:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8000002952 W\n",
            "[codecarbon INFO @ 13:26:35] Energy consumed for All CPU : 0.001447 kWh\n",
            "[codecarbon INFO @ 13:26:35] Energy consumed for all GPUs : 0.021296 kWh. Total GPU Power : 5.944428312993989 W\n",
            "[codecarbon INFO @ 13:26:35] 0.027038 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:26:50] Energy consumed for RAM : 0.004448 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:26:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000331200001 W\n",
            "[codecarbon INFO @ 13:26:50] Energy consumed for All CPU : 0.001499 kWh\n",
            "[codecarbon INFO @ 13:26:50] Energy consumed for all GPUs : 0.021320 kWh. Total GPU Power : 5.721974820662235 W\n",
            "[codecarbon INFO @ 13:26:50] 0.027267 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:27:05] Energy consumed for RAM : 0.004601 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:27:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800112226400001 W\n",
            "[codecarbon INFO @ 13:27:05] Energy consumed for All CPU : 0.001550 kWh\n",
            "[codecarbon INFO @ 13:27:05] Energy consumed for all GPUs : 0.021344 kWh. Total GPU Power : 5.840973244963039 W\n",
            "[codecarbon INFO @ 13:27:05] 0.027496 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:27:20] Energy consumed for RAM : 0.004754 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:27:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000388800001 W\n",
            "[codecarbon INFO @ 13:27:20] Energy consumed for All CPU : 0.001602 kWh\n",
            "[codecarbon INFO @ 13:27:20] Energy consumed for all GPUs : 0.021367 kWh. Total GPU Power : 5.463360149182876 W\n",
            "[codecarbon INFO @ 13:27:20] 0.027723 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:27:35] Energy consumed for RAM : 0.004907 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:27:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000396000003 W\n",
            "[codecarbon INFO @ 13:27:35] Energy consumed for All CPU : 0.001654 kWh\n",
            "[codecarbon INFO @ 13:27:35] Energy consumed for all GPUs : 0.021389 kWh. Total GPU Power : 5.432601688595963 W\n",
            "[codecarbon INFO @ 13:27:35] 0.027950 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:27:35] 0.001541 g.CO2eq/s mean an estimation of 48.60572620795268 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:27:50] Energy consumed for RAM : 0.005060 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:27:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000136800003 W\n",
            "[codecarbon INFO @ 13:27:50] Energy consumed for All CPU : 0.001705 kWh\n",
            "[codecarbon INFO @ 13:27:50] Energy consumed for all GPUs : 0.021411 kWh. Total GPU Power : 5.072650493898836 W\n",
            "[codecarbon INFO @ 13:27:50] 0.028176 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:28:05] Energy consumed for RAM : 0.005213 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:28:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800010159200001 W\n",
            "[codecarbon INFO @ 13:28:05] Energy consumed for All CPU : 0.001757 kWh\n",
            "[codecarbon INFO @ 13:28:05] Energy consumed for all GPUs : 0.021433 kWh. Total GPU Power : 5.418637015822395 W\n",
            "[codecarbon INFO @ 13:28:05] 0.028403 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:28:20] Energy consumed for RAM : 0.005366 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:28:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.800000273600002 W\n",
            "[codecarbon INFO @ 13:28:20] Energy consumed for All CPU : 0.001808 kWh\n",
            "[codecarbon INFO @ 13:28:20] Energy consumed for all GPUs : 0.021454 kWh. Total GPU Power : 4.906814793073714 W\n",
            "[codecarbon INFO @ 13:28:20] 0.028628 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:28:35] Energy consumed for RAM : 0.005519 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:28:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8000005328 W\n",
            "[codecarbon INFO @ 13:28:35] Energy consumed for All CPU : 0.001860 kWh\n",
            "[codecarbon INFO @ 13:28:35] Energy consumed for all GPUs : 0.021474 kWh. Total GPU Power : 4.788874514595955 W\n",
            "[codecarbon INFO @ 13:28:35] 0.028853 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:28:50] Energy consumed for RAM : 0.005672 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:28:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8005784192 W\n",
            "[codecarbon INFO @ 13:28:50] Energy consumed for All CPU : 0.001911 kWh\n",
            "[codecarbon INFO @ 13:28:50] Energy consumed for all GPUs : 0.021521 kWh. Total GPU Power : 11.262280030885766 W\n",
            "[codecarbon INFO @ 13:28:50] 0.029104 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:29:05] Energy consumed for RAM : 0.005825 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:29:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.813870022400002 W\n",
            "[codecarbon INFO @ 13:29:05] Energy consumed for All CPU : 0.001963 kWh\n",
            "[codecarbon INFO @ 13:29:05] Energy consumed for all GPUs : 0.021574 kWh. Total GPU Power : 12.732596040118304 W\n",
            "[codecarbon INFO @ 13:29:05] 0.029362 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:29:20] Energy consumed for RAM : 0.005978 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:29:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.810581012 W\n",
            "[codecarbon INFO @ 13:29:20] Energy consumed for All CPU : 0.002014 kWh\n",
            "[codecarbon INFO @ 13:29:20] Energy consumed for all GPUs : 0.021622 kWh. Total GPU Power : 11.732086895124086 W\n",
            "[codecarbon INFO @ 13:29:20] 0.029615 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:29:35] Energy consumed for RAM : 0.006131 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:29:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.804561632 W\n",
            "[codecarbon INFO @ 13:29:35] Energy consumed for All CPU : 0.002066 kWh\n",
            "[codecarbon INFO @ 13:29:35] Energy consumed for all GPUs : 0.021678 kWh. Total GPU Power : 13.248406230570016 W\n",
            "[codecarbon INFO @ 13:29:35] 0.029875 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:29:35] 0.001577 g.CO2eq/s mean an estimation of 49.74527929355334 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:29:50] Energy consumed for RAM : 0.006284 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:29:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.808766907200003 W\n",
            "[codecarbon INFO @ 13:29:50] Energy consumed for All CPU : 0.002118 kWh\n",
            "[codecarbon INFO @ 13:29:50] Energy consumed for all GPUs : 0.021721 kWh. Total GPU Power : 10.367727337730125 W\n",
            "[codecarbon INFO @ 13:29:50] 0.030123 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:30:05] Energy consumed for RAM : 0.006437 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:30:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8124945776 W\n",
            "[codecarbon INFO @ 13:30:05] Energy consumed for All CPU : 0.002169 kWh\n",
            "[codecarbon INFO @ 13:30:05] Energy consumed for all GPUs : 0.021771 kWh. Total GPU Power : 12.031374501625299 W\n",
            "[codecarbon INFO @ 13:30:05] 0.030378 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:30:20] Energy consumed for RAM : 0.006590 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:30:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8071385192 W\n",
            "[codecarbon INFO @ 13:30:20] Energy consumed for All CPU : 0.002221 kWh\n",
            "[codecarbon INFO @ 13:30:20] Energy consumed for all GPUs : 0.021821 kWh. Total GPU Power : 11.991758986244184 W\n",
            "[codecarbon INFO @ 13:30:20] 0.030632 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:30:35] Energy consumed for RAM : 0.006743 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:30:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.80728172 W\n",
            "[codecarbon INFO @ 13:30:35] Energy consumed for All CPU : 0.002272 kWh\n",
            "[codecarbon INFO @ 13:30:35] Energy consumed for all GPUs : 0.021874 kWh. Total GPU Power : 12.745290228996291 W\n",
            "[codecarbon INFO @ 13:30:35] 0.030890 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:30:50] Energy consumed for RAM : 0.006896 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:30:50] Delta energy consumed for CPU with cpu_load : 0.000053 kWh, power : 13.1174481368 W\n",
            "[codecarbon INFO @ 13:30:50] Energy consumed for All CPU : 0.002325 kWh\n",
            "[codecarbon INFO @ 13:30:50] Energy consumed for all GPUs : 0.021931 kWh. Total GPU Power : 13.78883100487541 W\n",
            "[codecarbon INFO @ 13:30:50] 0.031153 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:31:05] Energy consumed for RAM : 0.007049 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:31:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.820892434400001 W\n",
            "[codecarbon INFO @ 13:31:05] Energy consumed for All CPU : 0.002377 kWh\n",
            "[codecarbon INFO @ 13:31:05] Energy consumed for all GPUs : 0.021999 kWh. Total GPU Power : 16.23969301579957 W\n",
            "[codecarbon INFO @ 13:31:05] 0.031425 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:31:20] Energy consumed for RAM : 0.007202 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:31:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.825254669600001 W\n",
            "[codecarbon INFO @ 13:31:20] Energy consumed for All CPU : 0.002428 kWh\n",
            "[codecarbon INFO @ 13:31:20] Energy consumed for all GPUs : 0.022050 kWh. Total GPU Power : 12.226968508412016 W\n",
            "[codecarbon INFO @ 13:31:20] 0.031681 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:31:35] Energy consumed for RAM : 0.007355 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:31:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.804591202400001 W\n",
            "[codecarbon INFO @ 13:31:35] Energy consumed for All CPU : 0.002480 kWh\n",
            "[codecarbon INFO @ 13:31:35] Energy consumed for all GPUs : 0.022090 kWh. Total GPU Power : 9.505853647157183 W\n",
            "[codecarbon INFO @ 13:31:35] 0.031925 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:31:35] 0.001680 g.CO2eq/s mean an estimation of 52.985511718848784 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:31:50] Energy consumed for RAM : 0.007508 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:31:50] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.815568862400001 W\n",
            "[codecarbon INFO @ 13:31:50] Energy consumed for All CPU : 0.002532 kWh\n",
            "[codecarbon INFO @ 13:31:50] Energy consumed for all GPUs : 0.022146 kWh. Total GPU Power : 13.511636894757375 W\n",
            "[codecarbon INFO @ 13:31:50] 0.032186 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:32:05] Energy consumed for RAM : 0.007661 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:32:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.815265108800002 W\n",
            "[codecarbon INFO @ 13:32:05] Energy consumed for All CPU : 0.002583 kWh\n",
            "[codecarbon INFO @ 13:32:05] Energy consumed for all GPUs : 0.022207 kWh. Total GPU Power : 14.574481194984763 W\n",
            "[codecarbon INFO @ 13:32:05] 0.032451 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:32:20] Energy consumed for RAM : 0.007814 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:32:20] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.8074140128 W\n",
            "[codecarbon INFO @ 13:32:20] Energy consumed for All CPU : 0.002635 kWh\n",
            "[codecarbon INFO @ 13:32:20] Energy consumed for all GPUs : 0.022260 kWh. Total GPU Power : 12.905724571626333 W\n",
            "[codecarbon INFO @ 13:32:20] 0.032710 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:32:35] Energy consumed for RAM : 0.007967 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:32:35] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.810425542400003 W\n",
            "[codecarbon INFO @ 13:32:35] Energy consumed for All CPU : 0.002686 kWh\n",
            "[codecarbon INFO @ 13:32:35] Energy consumed for all GPUs : 0.022335 kWh. Total GPU Power : 17.84364023533062 W\n",
            "[codecarbon INFO @ 13:32:35] 0.032989 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:32:50] Energy consumed for RAM : 0.008121 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:32:50] Delta energy consumed for CPU with cpu_load : 0.000059 kWh, power : 14.756942979200003 W\n",
            "[codecarbon INFO @ 13:32:50] Energy consumed for All CPU : 0.002746 kWh\n",
            "[codecarbon INFO @ 13:32:50] Energy consumed for all GPUs : 0.022476 kWh. Total GPU Power : 33.96081298335169 W\n",
            "[codecarbon INFO @ 13:32:50] 0.033343 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 13:33:05] Energy consumed for RAM : 0.008274 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 13:33:05] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 12.801536364800002 W\n",
            "[codecarbon INFO @ 13:33:05] Energy consumed for All CPU : 0.002797 kWh\n",
            "[codecarbon INFO @ 13:33:05] Energy consumed for all GPUs : 0.022684 kWh. Total GPU Power : 49.86667715386758 W\n",
            "[codecarbon INFO @ 13:33:05] 0.033755 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "tracker = EmissionsTracker(output_file=train_code_carbon_out)\n",
        "tracker.start()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(\"\\n\\nTotal de emissões (detalhes em emissions.csv): \",emissions)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"\\nTempo total de execução: {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7fd421",
      "metadata": {},
      "source": [
        "# Avaliação final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7d0ffb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2246f446",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 4203\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tracker = EmissionsTracker(output_file=inference_code_carbon_out)\n",
        "tracker.start()\n",
        "preds = trainer.predict(test_ds)\n",
        "emissions = tracker.stop()\n",
        "print(f\"- Emissões durante a inferência: {emissions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86043802",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ALTA', 'BAIXA', 'MEDIA'], dtype=object)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoder.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0ce70c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ALTA       0.41      0.70      0.51       863\n",
            "       BAIXA       0.65      0.53      0.58      2056\n",
            "       MEDIA       0.36      0.28      0.32      1284\n",
            "\n",
            "    accuracy                           0.49      4203\n",
            "   macro avg       0.47      0.51      0.47      4203\n",
            "weighted avg       0.51      0.49      0.49      4203\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_ds['labels'], preds.predictions.argmax(axis=-1), target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73ecd6f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.70      0.51       863\n",
            "           1       0.65      0.53      0.58      2056\n",
            "           2       0.36      0.28      0.32      1284\n",
            "\n",
            "    accuracy                           0.49      4203\n",
            "   macro avg       0.47      0.51      0.47      4203\n",
            "weighted avg       0.51      0.49      0.49      4203\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_ds['labels'], preds.predictions.argmax(axis=-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b393a4b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bonito",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e046339f93946889a718fc04abb72d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1337d826cc1c432fb0ff426f76aa0a40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e619f6680440adb94aa980b1e89f97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d770cacc1c643048bd07fb695d9feab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c901c88a5184cf49be3c99d2130d994",
            "max": 7999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f2c3f88e34f421ca1c2f724b9a145f9",
            "value": 7999
          }
        },
        "2eee9befa38348a58e9e1f89be177aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3128da4fe1c64711ae51801d3ff854e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b279d9b02f4a0e9f670243083008b0",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e668ba65bdc342b5b749c7146f6f9f27",
            "value": 2000
          }
        },
        "43a1c8c0ea054197a64423b736920a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c222a8bf0073423a8065d3ec330d5ee8",
            "placeholder": "​",
            "style": "IPY_MODEL_833a450e4a914a68beeeac27ef2e808c",
            "value": "Map: 100%"
          }
        },
        "6eb3b183113d42f5a44dcec5cbd1bb89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4626f82fbe5407ba5fba7f63cfec5f6",
            "placeholder": "​",
            "style": "IPY_MODEL_2eee9befa38348a58e9e1f89be177aa5",
            "value": "Map: 100%"
          }
        },
        "6f3e73f3dad441cdbd7e94497543fa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eb3b183113d42f5a44dcec5cbd1bb89",
              "IPY_MODEL_3128da4fe1c64711ae51801d3ff854e2",
              "IPY_MODEL_73a96be96b074ad5997ae68b5e2eb8ce"
            ],
            "layout": "IPY_MODEL_0e046339f93946889a718fc04abb72d2"
          }
        },
        "73a96be96b074ad5997ae68b5e2eb8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e619f6680440adb94aa980b1e89f97",
            "placeholder": "​",
            "style": "IPY_MODEL_aeec25216770453cbf08207b00eb58a1",
            "value": " 2000/2000 [00:03&lt;00:00, 541.69 examples/s]"
          }
        },
        "7c901c88a5184cf49be3c99d2130d994": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2c3f88e34f421ca1c2f724b9a145f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "833a450e4a914a68beeeac27ef2e808c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6da50e3375f4dcdb9e7975e8fe07813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeec25216770453cbf08207b00eb58a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c222a8bf0073423a8065d3ec330d5ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc66082c62b490fa51a30de84fb93ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea7e6f921aa4311bdfd10973dc80211",
            "placeholder": "​",
            "style": "IPY_MODEL_a6da50e3375f4dcdb9e7975e8fe07813",
            "value": " 7999/7999 [00:24&lt;00:00, 393.98 examples/s]"
          }
        },
        "e4626f82fbe5407ba5fba7f63cfec5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e668ba65bdc342b5b749c7146f6f9f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6b279d9b02f4a0e9f670243083008b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea7e6f921aa4311bdfd10973dc80211": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2a334c9801427bad43d0f577909373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43a1c8c0ea054197a64423b736920a8e",
              "IPY_MODEL_2d770cacc1c643048bd07fb695d9feab",
              "IPY_MODEL_dbc66082c62b490fa51a30de84fb93ed"
            ],
            "layout": "IPY_MODEL_1337d826cc1c432fb0ff426f76aa0a40"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
